# -*- coding: utf-8 -*-
"""tf_idf.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vWm2VXcx_H4r2WIreeuUhqgTko_cynYV

#Comparar nubes de palabras usando tf-idf

## **Librerias**
"""

import pandas as pd
import matplotlib.pyplot as plt 
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
import nltk
from PIL import Image
import numpy as np
import nltk
import re

!pip install -q wordcloud
import wordcloud
import nltk
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans
from sklearn.decomposition import NMF, LatentDirichletAllocation
from sklearn.decomposition import PCA
from sklearn.manifold import MDS

"""##**Módulos**"""

#========= MODULOS =================
#JUNTA UN ARRAY DE STRING'S EN UN SOLO STRING
def juntar(arr):
    text=''
    for x in range(0,len(arr)):
        text=text+arr[x]
    return text

def hacer_corpus(tweets):
    corpus = []
    for i in range(0, len(tweets)):
        #print(i)
        #eliminamos los links y hasgtags
        tweets['text'][i] = " ".join([word for word in tweets['text'][i].split()
                                        if 'http' not in word and '@' not in word and '#' not in word])
        #definimos que caracteres que pueden leer
        title = re.sub('[^a-zA-Z_á_é_í_ó_ú_ñ_Á_É_Í_Ó_Ú]', ' ', tweets['text'][i])
        #transformamos las mayusculas en minusculas
        title = title.lower()
        #generamos los arreglos respectivos para los tweets con los carcteres aceptados
        title = title.split()
        #print(title)
        #eliminamos aquellas palabras que esten en el stopwords del español
        title = [word for word in title if not word in stopwords.words('spanish')]
        #volvemos a crear el corpus con las palabras aceptadas
        title = ' '.join(title)
        corpus.append(title)
    return corpus

"""## 1.  Carga y procesado texto (data set)"""

#descarga de pacquete de palabras para omitir
nltk.download('stopwords')
stopwords.words('spanish')

#leemos el archivo CSV previamente extraidos de twitter
tweets1=hacer_corpus(pd.read_csv('PoliciaPeru.csv'))
tweets2=hacer_corpus(pd.read_csv('SeleccionPeru.csv')) 
#volvemos un string
Bow1=juntar(tweets1)
Bow2=juntar(tweets2)
#print(Bow1)
#print(Bow2)

# Setup
pd.set_option("display.precision", 4)
# Cargamos dataset
dat = [
         ['X', Bow1],
         ['Y', Bow2],
      ]
df_sentences = pd.DataFrame(dat, columns=['Etiqueta', 'Contenido'])

labels = df_sentences['Etiqueta']
print("Etiquetas:")
print(labels)

n_clusters = np.unique(labels).shape[0]
print("n_clusters:", n_clusters)

"""## 2. Vectorizar texto en una matriz numérica"""

# Build tf-idf vectorizer and cosine similarity matrix
def build_vectorizer(sentences, vocab=None, min_df=0.0, max_df=1.0, ngram_range=(1,1)):   # for a 2-gram use: ngram_range=(1,2)
    '''
    Construya el vectorizador tf-idf:
     1. Construya el count_vectorizer a partir del data set de entrada.
     2. Transforme count_vectorizer en bolsa de palabras.
     3. Adapte la transformación a la bolsa de palabras.
    '''
    
    # Construimos count vectorizer
    count_vectorizer = CountVectorizer(max_df=max_df, min_df=min_df, vocabulary=vocab, ngram_range=(1,1))  # stop_words='english, max_features=N_FEATURES 
    cvec = count_vectorizer.fit(sentences)

    # Obtener nombres de funciones
    feature_names = cvec.get_feature_names()

    # Obtener bolsa de palabras y analizar
    bag_of_words = cvec.transform(sentences)
    df_bag_of_words = pd.DataFrame(bag_of_words.todense(), columns=feature_names)
    
    # Transforma bag_of_words en matriz tf-idf
    transformer = TfidfTransformer()
    tfidf = transformer.fit_transform(bag_of_words)

    # Encuentra las palabras más populares y los pesos más altos
    word_cnts = np.asarray(bag_of_words.sum(axis=0)).ravel().tolist()  # for each word in column, sum all row counts
    df_cnts = pd.DataFrame({'word': feature_names, 'count': word_cnts})
    df_cnts = df_cnts.sort_values('count', ascending=False)

    # Cree pesos de palabras como una lista y ordénelos
    weights = np.asarray(tfidf.mean(axis=0)).ravel().tolist()
    df_weights = pd.DataFrame({'word': feature_names, 'weight': weights})
    df_weights = df_weights.sort_values('weight', ascending=False)

    df_weights = df_weights.merge(df_cnts, on='word', how='left')
    df_weights = df_weights[['word', 'count', 'weight']]

    # Similitud de oraciones
    cos_sim = cosine_similarity(tfidf, tfidf)

    # Matriz de distancia de oraciones
    samp_dist = 1 - cos_sim

    return cvec, feature_names, df_bag_of_words, tfidf, df_weights, cos_sim, samp_dist
  
# Construccion
sentences = df_sentences['Contenido'].values.tolist()
cvec, feature_names, df_bag_of_words, tfidf, df_weights, cos_sim, samp_dist = build_vectorizer(sentences)

df_tfidf = pd.DataFrame(tfidf.todense(), columns=feature_names)

print("%d data set:" % len(sentences))
print(sentences)
print("---")
print("%d numero de palabras (cada palabra)" % len(feature_names))
print(feature_names)
print("---")
print("df_tfidf[%d,%d]:" % (len(sentences), len(feature_names)))
print(df_tfidf.to_string())
print("---")
print("df_weights:")
print(df_weights)

"""## 3. Bag-of-words"""

print("df_bag_of_words[%d,%d]:" % (len(sentences), len(feature_names)))
print(df_bag_of_words)

"""##4. Construccion de una nube de palabras a partir de las palabras ponderadas"""

#Promedios - palabra - n-veces - peso
print(df_weights)
#creacion de nube
s_word_freq = pd.Series(df_weights['count'])
s_word_freq.index = df_weights['word']
di_word_freq = s_word_freq.to_dict()

#Impresion de frecuencias de palabras
print("Frecuencia de palabras:")
for k,v in di_word_freq.items():
  print(k,v)

#Impresion de la nube de palabras
cloud = wordcloud.WordCloud(width=900, height=500).generate_from_frequencies(di_word_freq)
plt.imshow(cloud)
plt.axis('off')
plt.show()