# -*- coding: utf-8 -*-
"""Copia de TFIDP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_i6GfOCy4klnVa3mREd94hYXRxnm65A2

# Librerias
"""

import pandas as pd
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
import matplotlib.pyplot as plt 
from PIL import Image
import numpy as np
import nltk
import re
import math
from nltk.corpus import stopwords
nltk.download('stopwords')
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

"""# **No guarda los tweets de las tendencias que sean # **
#**No guarda en español **
#**No guarda el numero indicado(limitacion) **
"""





import tweepy
import json
import time
import pandas as pd
from datetime import date, datetime
import os
import csv
consumer_key ="kNPRvUTbdqzJZyYFxceJ8O4q4"    
consumer_secret ="Xf5bFeMlVkvlOhDs4RycJkBujRMKjFAuBY9a6964Oee0DNHkDV"
access_token ="1407121689264152579-FlJRSHzHs6JX4QsdUMqpplInsELjMK"
access_token_secret ="q2u1629kx9kpsto7ITlCSjes7reuo3zvgXVerB5dqGPpE"

auth=tweepy.OAuthHandler(consumer_key,consumer_secret)
auth.set_access_token(access_token,access_token_secret)

api= tweepy.API(auth,wait_on_rate_limit=True, wait_on_rate_limit_notify=True)   

#Obtener informacion de uno mismo
def unoMismo():
    data=api.me()
    print(json.dumps(data._json,indent=2))
#Obtener informacion de alguna cuenta
def Tercero(nombre):
    data=api.get_user(nombre)
    print(json.dumps(data._json,indent=2))
#Obtener seguidores
def Seguidores(nombre,cantidad):
    for usuario in tweepy.Cursor(api.followers,screen_name=nombre).items(cantidad):
        print(json.dumps(usuario._json,indent=2))
#Obtener personas que sigue una cuenta
def Seguidos(nombre,cantidad):
    for usuario in tweepy.Cursor(api.friends,screen_name=nombre).items(cantidad):
        print(json.dumps(usuario._json,indent=2))
#Obter un timeline de tweets
def TimeLine(nombre,cantidad):
    for tweet in tweepy.Cursor(api.user_timeline,screen_name=nombre,tweet_mode="extended").items(cantidad):
        print(json.dumps(tweet._json,indent=2))
        print("============================================================")
#Obtener informacion sobre un tweet en especifico
def Tweets(consulta,cantidad):
    for tweet in tweepy.Cursor(api.search,q=consulta,tweet_mode="extended").items(cantidad):
        print(json.dumps(tweet._json,indent=2))
#Obtener informacion sobre las tendencias
def TopTrends(woeid):
    trends = api.trends_place(id = woeid)
    for value in trends:
        for trend in value['trends']:
            print(trend['name'])
#Funcion que retorna una variable
def RetornaTopTrends(woeid):
    a = []
    trends = api.trends_place(id = woeid)
    for value in trends:
        for trend in value['trends']:
            a.append(trend['name'])
    return a
# TopTrends(418440) #woeid peru
def BuscarUsuarios(nombre, cantidad):
    for usuarios in tweepy.Cursor(api.search_users,q = nombre).items(cantidad):
        print(json.dumps(usuarios._json, indent = 2))

# BuscarUsuarios("geeksforgeeks",5)

#==============================================================================
def almacenar_tweet( status,nombre):
    csvFile = open(nombre, 'a', newline='',encoding='utf-8')
    csvWriter = csv.writer(csvFile, delimiter=';')
    if status is not False and status.text is not None:
        try:
            texto = status.extended_tweet["full_text"]
        except AttributeError:
            texto = status.text
        print(texto)


        linea = [status.created_at,
                    status.id, texto, status.source, status.truncated,
                    status.in_reply_to_status_id, status.in_reply_to_user_id,
                    status.in_reply_to_screen_name, status.geo, status.coordinates,
                    status.place,status.contributors, status.lang, status.retweeted]
        linea = linea
        csvWriter.writerow(linea)
    
    print("Almacenamos tweet")
    csvFile.close()
#==================================================================
def guardar(tema,nombre,cantidad,inicio,fin):
    print("===== Captador de tweets =====")
    # Determinamos si existe el fichero
    if os.path.isfile(nombre):
        print('Preparado el fichero')
    #Si no existe se crea uno nuevo
    else:
        print('El no archivo existe.')
        csvFile = open(nombre, 'w', newline='')
        csvWriter = csv.writer(csvFile)
        cabecera=['Fecha_creación','Id','Texto','Fuente','Truncado'
            ,'Respuesta_al_tweet','Respuesta_al_usuario_id'
            ,'Respuesta_al_usuario_nombre'

        ]
        csvWriter.writerow(cabecera)
        csvFile.close()
        print("Creación de la cabecera")

    

    for tweet in tweepy.Cursor(api.search, q=tema ,
                                       lang="es",
                                       since=inicio ,until=fin ).items(cantidad):
                print(tweet.created_at, tweet.text)
                almacenar_tweet(tweet,nombre)


    # End
    print("Terminado")
#===============================================

fecha_inicio = datetime(2019, 9, 19, 12, 00, 00)
fecha_fin =date.today()
#guardar("xbox","datos2.csv",50,fecha_inicio,fecha_fin)
def guardarVarios(arregloDeElementos,cantidad,inicio,fin):
    for elemento in arregloDeElementos:
        titulo="Datos"+elemento+".csv"
        guardar(elemento,titulo,cantidad,inicio,fin)
      
        print("===============DATOS ALMACENADOS===============")
guardarVarios(RetornaTopTrends(418440),1000,fecha_inicio,fecha_fin)

"""# Lectura de datos"""

#leemos el archivo CSV
tweets=pd.read_csv('Keiko.csv')

"""#Stop words en el español"""

print("STOPWORDS SPAÑOL")
print(stopwords.words('spanish'))

"""# Preparacion de datos"""

### Porcesando DATA SET
def prepararDatos(datos):
  corpus = []
  for i in range(len(datos)):
      
      #eliminamos los links y hasgtags
      datos['text'][i] = " ".join([word for word in datos['text'][i].split()
                                    if 'http' not in word and '@' not in word and '#' not in word])
      #definimos que caracteres que pueden leer
      title = re.sub('[^a-zA-Z_á_é_í_ó_ú_ñ_Á_É_Í_Ó_Ú]', ' ', datos['text'][i])
      #transformamos las mayusculas en minusculas
      title = title.lower()
      #generamos los arreglos respectivos para los tweets con los carcteres aceptados
      title = title.split()
      #eliminamos aquellas palabras que esten en el stopwords del español
      title = [word for word in title if not word in stopwords.words('spanish')]
      #volvemos a crear el corpus con las palabras aceptadas
      title = ' '.join(title)
      corpus.append(title)
  return corpus

corpus=prepararDatos(tweets)
corpus

"""# Creacion del bag of words

## Creacion de los tokens utilizados
"""

#Creando el Bag of Words
#crea un diccionario con todos los tokens
bow_article = CountVectorizer().fit(corpus)
#crea una lista de los tokens
count_tokens=bow_article.get_feature_names()
#se crea la matriz para el bag of words
article_vect = bow_article.transform(corpus)
#mostramos lo tokens
count_tokens

"""## Creacion de la matriz"""

#Mostramos al matriz
df_count_vect=pd.DataFrame(data=article_vect.toarray(),columns=count_tokens)
df_count_vect

aux=""
for i in corpus:
  aux=aux+i
aux2=[aux]
#Creando el Bag of Words
#crea un diccionario con todos los tokens
bow_article = CountVectorizer().fit(aux2)
#crea una lista de los tokens
count_tokens=bow_article.get_feature_names()
#se crea la matriz para el bag of words
article_vect = bow_article.transform(aux2)
#mostramos lo tokens
count_tokens
#Mostramos al matriz
df_count_vect=pd.DataFrame(data=article_vect.toarray(),columns=count_tokens)
df_count_vect

"""#===================================

# TF IDf

## Numero de palabras por documento
"""

def numeroPalabras(datos):
  uniqueWords =[]
  numeros=[]
  for dato in datos:
    uniqueWords=set(uniqueWords).union(set(dato))
  
  for dato in datos:
    numOfWordsAux = dict.fromkeys(uniqueWords, 0)
    for palabra in dato:
      numOfWordsAux[palabra]+=1
    numeros.append(numOfWordsAux)
  return numeros

"""## Calculo de TF"""

def calculoTF(numero,datos):
  TF=[]
  for i in range(len(datos)):
    tfDiccionario={}
    contadorBagOfWords=len(datos[i])
    for palabra, count in numero[i].items():
      tfDiccionario[palabra] = count / float(contadorBagOfWords)
    TF.append(tfDiccionario)
  return TF

"""## Calculo de IDF"""

def calculoIDF(numero):
  N = len(numero)
  idfDiccionario = dict.fromkeys(numero[0].keys(), 0)
  for documento in numero:
      for word, val in documento.items():
          if val > 0:
              idfDiccionario[word] += 1
  for word, val in idfDiccionario.items():
      idfDiccionario[word] = math.log(N / float(val))
  return idfDiccionario

"""## Calculo de TF IDF"""

def calculoTFIDF(GrupotfBagOfWords, idfs):
  TFIDF=[]
  for tfBagOfWords in GrupotfBagOfWords:
    tfidf = {}
    for word, val in tfBagOfWords.items():
        tfidf[word] = val * idfs[word]
    TFIDF.append(tfidf)
  return TFIDF

"""## Mostrar Datos"""

def Calculo(datos):
  datos2=[]
  for i in range (len( datos)):
    dato=datos[i].split()
    datos2.append(dato)
  numero=numeroPalabras(datos2)
  TF=calculoTF(numero,datos2)
  IDF=calculoIDF(numero)
  TFIDF=calculoTFIDF(TF,IDF)
  df=pd.DataFrame(TFIDF)
  #for i in numero:
    #print(i)
  #print(df)
  return df

#Ejemplo
documentA = 'hola señora'
documentB = 'como estas señora buenos días'
documentC= "vieja buenos dias"
bagOfWordsA = documentA.split(' ')
bagOfWordsB = documentB.split(' ')
datos=[documentA,documentB,documentC]
Calculo(datos)

"""## Trabajo con datos reales

### Lectura de datos
"""

#leemos el archivo CSV
consejoDeMinistros=pd.read_csv('pcmperu.csv')

"""### Limpieza de datos"""

corpus=prepararDatos(consejoDeMinistros)
corpus

"""### Procesamos"""

Calculo(corpus)

# Python program to generate WordCloud

# importing all necessary modules
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
import pandas as pd

# Reads 'Youtube04-Eminem.csv' file
df = pd.read_csv(r"pcmperu.csv", encoding ="latin-1")

comment_words = ''
stopwords = set(STOPWORDS)

# iterate through the csv file
for val in df.CONTENT:
	
	# typecaste each val to string
	val = str(val)

	# split the value
	tokens = val.split()
	
	# Converts each token into lowercase
	for i in range(len(tokens)):
		tokens[i] = tokens[i].lower()
	
	comment_words += " ".join(tokens)+" "

wordcloud = WordCloud(width = 800, height = 800,
				background_color ='white',
				stopwords = stopwords,
				min_font_size = 10).generate(comment_words)

# plot the WordCloud image					
plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)

plt.show()